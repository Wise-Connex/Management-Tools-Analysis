#!/usr/bin/env python3
"""
Key Findings Streaming Console Test

Tests the Key Findings generation process with detailed console streaming
to show exactly what's happening at each step.
"""

import asyncio
import time
import logging
import os
import sys
from datetime import datetime

# Add the project root to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Configure detailed logging to see everything
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('key_findings_streaming.log')
    ]
)

async def test_key_findings_streaming():
    """Test Key Findings generation with full streaming output"""
    print("ğŸ¬ STARTING KEY FINDINGS STREAMING TEST")
    print(f"ğŸ•’ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)

    try:
        # Import required modules
        print("ğŸ“¦ Importing modules...")
        from database import get_database_manager
        from dashboard_app.key_findings.data_aggregator import DataAggregator
        from dashboard_app.key_findings.prompt_engineer import PromptEngineer
        from dashboard_app.key_findings.ai_service import get_openrouter_service
        print("âœ… Modules imported successfully")

        # Check API key
        api_key = os.getenv('OPENROUTER_API_KEY')
        if not api_key:
            print("âŒ OPENROUTER_API_KEY not found!")
            print("   Please set your OpenRouter API key to test AI generation")
            return

        print(f"ğŸ”‘ API key found: {api_key[:10]}...")

        # Initialize components
        print("ğŸ”§ Initializing components...")
        db_manager = get_database_manager()
        ai_service = get_openrouter_service(api_key)
        data_aggregator = DataAggregator(db_manager, None)
        prompt_engineer = PromptEngineer('es')
        print("âœ… Components initialized")

        # Test with sample data
        test_tool = "Benchmarking"
        test_sources = ["Google Trends", "Google Books", "Crossref"]

        print(f"\nğŸ§ª Testing Key Findings generation for '{test_tool}'")
        print(f"ğŸ“‹ Sources: {test_sources}")

        # Step 1: Data Collection
        print(f"\nğŸ“Š STEP 1: Data Collection")
        print("-" * 40)
        collection_start = time.time()

        analysis_data = data_aggregator.collect_analysis_data(
            tool_name=test_tool,
            selected_sources=test_sources,
            language='es'
        )

        collection_time = time.time() - collection_start

        if 'error' in analysis_data:
            print(f"âŒ Data collection failed: {analysis_data['error']}")
            return

        print(f"âœ… Data collection completed in {collection_time:.2f}s")
        print(f"   â”œâ”€â”€ Data points: {analysis_data.get('data_points_analyzed', 0):,}")
        print(f"   â”œâ”€â”€ PCA variance: {analysis_data.get('pca_insights', {}).get('total_variance_explained', 0):.1f}%")
        print(f"   â””â”€â”€ Performance metrics: {analysis_data.get('performance_metrics', {})}")

        # Step 2: Prompt Generation
        print(f"\nğŸ“ STEP 2: Prompt Generation")
        print("-" * 40)
        prompt_start = time.time()

        prompt = prompt_engineer.create_analysis_prompt(analysis_data, {})

        prompt_time = time.time() - prompt_start
        print(f"âœ… Prompt generated in {prompt_time:.2f}s")
        print(f"   â”œâ”€â”€ Prompt length: {len(prompt)} characters")
        print(f"   â”œâ”€â”€ Estimated tokens: ~{len(prompt)//4}")
        print(f"   â””â”€â”€ Language: {prompt_engineer.language}")

        # Show prompt preview
        prompt_preview = prompt[:500] + "..." if len(prompt) > 500 else prompt
        print(f"\nğŸ“‹ PROMPT PREVIEW:")
        print(f"{prompt_preview}")

        # Step 3: AI Analysis
        print(f"\nğŸ¤– STEP 3: AI Analysis")
        print("-" * 40)
        ai_start = time.time()

        print(f"ğŸš€ Calling AI service...")
        ai_response = ai_service.generate_analysis(
            prompt=prompt,
            language='es'
        )

        ai_time = time.time() - ai_start

        if not ai_response.get('success', False):
            print(f"âŒ AI analysis failed: {ai_response}")
            return

        print(f"âœ… AI analysis completed in {ai_time:.2f}s")
        print(f"   â”œâ”€â”€ Model used: {ai_response.get('model_used', 'unknown')}")
        print(f"   â”œâ”€â”€ Response time: {ai_response.get('response_time_ms', 0)}ms")
        print(f"   â”œâ”€â”€ Tokens processed: {ai_response.get('token_count', 0)}")
        print(f"   â””â”€â”€ Language: {ai_response.get('language', 'unknown')}")

        # Step 4: Response Processing
        print(f"\nğŸ“„ STEP 4: Response Processing")
        print("-" * 40)
        ai_content = ai_response.get('content', {})

        print(f"ğŸ“Š Response parsed:")
        print(f"   â”œâ”€â”€ Principal findings: {len(ai_content.get('principal_findings', []))}")
        print(f"   â”œâ”€â”€ PCA insights: {len(ai_content.get('pca_insights', {}))}")
        print(f"   â””â”€â”€ Executive summary: {len(ai_content.get('executive_summary', ''))} characters")

        # Show response preview
        if ai_content.get('principal_findings'):
            print(f"\nğŸ” FIRST FINDING PREVIEW:")
            first_finding = ai_content['principal_findings'][0]
            print(f"   Bullet point: {first_finding.get('bullet_point', '')[:200]}...")
            print(f"   Confidence: {first_finding.get('confidence', 'unknown')}")
            print(f"   Data sources: {first_finding.get('data_source', [])}")

        if ai_content.get('executive_summary'):
            print(f"\nğŸ“‹ EXECUTIVE SUMMARY PREVIEW:")
            summary_preview = ai_content['executive_summary'][:300] + "..." if len(ai_content['executive_summary']) > 300 else ai_content['executive_summary']
            print(f"   {summary_preview}")

        # Final Summary
        total_time = collection_time + prompt_time + ai_time
        print(f"\nğŸ FINAL SUMMARY")
        print("-" * 40)
        print(f"â±ï¸ Total processing time: {total_time:.2f}s")
        print(f"   â”œâ”€â”€ Data collection: {collection_time:.2f}s ({collection_time/total_time*100:.1f}%)")
        print(f"   â”œâ”€â”€ Prompt generation: {prompt_time:.2f}s ({prompt_time/total_time*100:.1f}%)")
        print(f"   â””â”€â”€ AI analysis: {ai_time:.2f}s ({ai_time/total_time*100:.1f}%)")

        print(f"\nâœ… STREAMING TEST COMPLETED SUCCESSFULLY!")
        print(f"ğŸ•’ Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“„ Detailed logs in: key_findings_streaming.log")

    except Exception as e:
        print(f"\nâŒ STREAMING TEST FAILED: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_key_findings_streaming())